# Influence Functions in NLP: Understanding and Mitigating Data Poisoning

This repository contains the work completed as part of the **final project for COMS 4774 (Unsupervised Learning)** at Columbia University. The project investigates the use of **influence functions** in Natural Language Processing (NLP), exploring their applications for understanding black-box model behavior and defending against data poisoning attacks.

## ðŸ“„ About the Paper
- **Title**: *Influence Functions in NLP*  
- **Authors**: Rene Sultan, Nikos Goutzoulias, Arnaud Lamy, Yuval Shemla  
- **Date**: December 13, 2024  

### Summary
The paper addresses the challenges of **interpretability** in modern machine learning, with a focus on large language models (LLMs). It explores how influence functionsâ€”originally a tool from robust statisticsâ€”can be applied to:
1. Understand the impact of training data on test-time predictions.
2. Identify influential or mislabeled training examples.
3. Develop defenses against **data poisoning attacks**, where adversarial data is injected into the training set to degrade model performance.

### Resources
- **White Paper**: The white paper is uploaded above as a pdf.  
- **Implementations**: The Notebooks and the code files can be found in this repository:  
  [https://github.com/goutzou/Data-Poisoning](https://github.com/goutzou/Data-Poisoning)
